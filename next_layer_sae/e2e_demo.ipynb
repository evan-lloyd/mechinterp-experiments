{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d465fbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://colab.research.google.com/github/evan-lloyd/mechinterp-experiments/blob/main/next_layer_sae/e2e_demo.ipynb\" target=\"_blank\"><button style=\"background-color: #4285f4; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; font-size: 14px;\">Run this notebook in Google Colab</button></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(() => {\n            if(google.colab) {\n                debugger;\n                google.colab.notebook.cells[0].dispose();\n            }\n        }, 0);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hack to display a button to open in Colab, which is removed if we already are\n",
    "\n",
    "from IPython.display import HTML, display, Javascript\n",
    "display(\n",
    "    HTML(\n",
    "        '<a href=\"https://colab.research.google.com/github/evan-lloyd/mechinterp-experiments/blob/main/next_layer_sae/e2e_demo.ipynb\" target=\"_blank\"><button style=\"background-color: #4285f4; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; font-size: 14px;\">Run this notebook in Google Colab</button></a>'\n",
    "    )\n",
    ")\n",
    "display(\n",
    "    Javascript(\"\"\"\n",
    "        setTimeout(() => {\n",
    "            if(google.colab) {\n",
    "                debugger;\n",
    "                google.colab.notebook.cells[0].dispose();\n",
    "            }\n",
    "        }, 0);\n",
    "        \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03231229",
   "metadata": {},
   "source": [
    "# Initialize notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If we're running in Colab, we need to clone the non-notebook source from git.\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\") and not os.path.isdir(\n",
    "    \"/content/mechinterp-experiments\"\n",
    "):\n",
    "    ip = get_ipython()  #  pyright: ignore[reportUndefinedVariable]\n",
    "    ip.run_cell_magic(\n",
    "        \"bash\",\n",
    "        \"\",\n",
    "        \"\"\"\n",
    "    git clone --filter=blob:none --no-checkout https://github.com/evan-lloyd/mechinterp-experiments.git\n",
    "    cd mechinterp-experiments\n",
    "    git sparse-checkout init --no-cone\n",
    "    echo \"/next_layer_sae\" > .git/info/sparse-checkout\n",
    "    git checkout\n",
    "  \"\"\",\n",
    "    )\n",
    "    ip.run_line_magic(\"cd\", \"mechinterp-experiments/next_layer_sae\")\n",
    "else:\n",
    "    print(\"Already cloned source, or not running in Colab.\")\n",
    "\n",
    "# Nice for dev, but not needed for Colab\n",
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ab8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab753f0",
   "metadata": {},
   "source": [
    "# Train SAEs for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from next_layer_sae.sae import SAE\n",
    "\n",
    "next_layer_saes = {}\n",
    "e2e_saes = {}\n",
    "finetuned_saes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6b425",
   "metadata": {},
   "source": [
    "## Next-layer auxiliary loss (my method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c39f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4038ed6e",
   "metadata": {},
   "source": [
    "## Full end-to-end training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2ff90",
   "metadata": {},
   "source": [
    "## End-to-end fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce323e",
   "metadata": {},
   "source": [
    "# Evaluations and comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
